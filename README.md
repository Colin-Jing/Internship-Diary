# Internship-Diary

1、打开软件、连好硬件
2、修改控制realsence的代码

现在就是感觉代码中有些参数很没有必要啊，也许是有些参数是为下一级准备的吧
但是现在俩看有些参数就是很混乱。



# 2022.6.20
委托是一个类，传给委托函数，就创建了委托实例
给实例传递函数的参数，就调用了函数。

在两个不能直接调用的方法中作为桥梁,如:在多线程中的跨线程的方法调用就得用委托

单独骨骼的部分我觉得不需要给医生展示呀。

左边ROI角度与右边ROI角度是什么意思？？左边的一个XBOx和右边的一个Xbox吗？
正面和侧面的角度都需要对应的ROI数组来调整吧。

接下来就是判断AI模型返回的结果。python程序保存的是CSV文件和效果图。
但是项目中只用到了CSV文件，根据CSV文件还对结果进行了后续修改。
效果图没什么用，就是给用户看个大概的。

这很多参数是真的乱啊，定义了在程序中很多是临时乱用的。

	
只要连接了探测板就不能debug。
连接树莓派先要打开它。
cd rs_FDR
sudo ./run

# 2022.6.21
### 0级demo任务：
1. 彩色图放中间点（√）
2. 全身图右边窗口删除，全身图窗口往中间放（√）
3. AI弹出窗口图加ROI区域（√）
4. 脊柱图放大（√）

### 1级demo任务：
1. realsense解决（待定）（realsense还没好，那这部分的时序就先不管）
2. 线性模组正常移动
4. 各种时序正确（即等硬件完成，软件再动。）
3. 探测板有图回来，时序对齐，图回来之后再调用本地图。
4. ROI区域正确驱动限速器。

### 另外DCT任务，设计一个窗口。




另外我觉得有必要把每个步骤代码做了什么梳理一下，不然真的是乱啊。

python进程不把上次的kill了容易导致错误。

上面任务3刚好过一遍全局的：
1. 各种初始化
2. 点击开始按钮，进行外体态扫描
3. 外体态扫描时通知realsense进行彩图和点云图
4. 设置MB通信参数，同时传递电流电压等参数给控制正面和侧面的树莓派
5. 等待realsense图像返回（但是为啥没有看到线性模组移动的代码，难道是在MB通信代码里面？！！）
6. 获取图像位移相关的返回值，便于图像拼接。
7. 之后就进行图像拼接，从socket获取返回值。
8. 外体态扫描完毕，等待用户设置限位值，设置完之后点击曝光确定按钮。
9. 点击确定之后进入case2，获取限位值，根据限位值计算初扫帧数和细扫帧数，进入case3。
10. 将限位通过MB参数寄存机传给主控板，接下来就全权交给程序来运行。
11. 进入step4，先设置本地图像的一些路径，demo01的路径下是确定的图像。
12. 然后从探测板取图，demo1要取，但是不要用。
   （但是为什么自始至终都没有看到线性模组在运动？
   主控板连接着线性模组，主控板已经烧录好了程序，
   难道是一开始我只给主控板传递线性模组运动参数？
   只等我给它信号，它就会按照固定的速度运行？！！
   那么是什么时候给的主控板线性模组的运动参数呢？！！）
13. 等探测板给我传回数据，传回一个就写一个到本地。并且不断与上一张进行拼接，产生缩略图展示。
14. 当图片数量达到初扫帧数之后就启动AI来进行脊柱判别。（探测板取到图就会将数据放到_rawImages里面。）
15. 启动完ai告诉探测板不用继续取图了。
16. 检查AI是否完成，如果完成，打开标签文件，计算ROI区域并弹窗展示。
17. 完成之后进入step5，根据ROI计算线束器角度，并传给主控板。
    这里demo1应该不计算也行，不计算就是拍的位置不太对。
18. 之后进入step6，最终的细扫阶段。
19. 细扫也是接收一帧数据就保存本地，然后与之前图片进行拼接，并展示预览图。
20. 结束。


应该就是线性模组自己运动，代码运行时需要什么就先等待硬件传递什么，
传递之后就进行处理，处理完毕就再等硬件。（所以硬件不能太快，至少不能比软件快。
但是或许可以比软件快，如果传回来的值不互相覆盖就行。但是好像又不行，
因为硬件后面的一些调整要软件传过来的结果的。）

_fdr_ctl_param.nFramePreview 初扫帧数
_fdr_ctl_param.nFrameHighQ 细扫帧数

# 2022.6.22

初扫和细扫的间隔都是确定的。
所以是根据限位来，通过加入间隔的计算来确定要拍多少帧。


demo01我不应该在代码里面确定它需要多少帧，因为代码很乱，我应该设置限位来让它有多少帧。

5帧 和 10帧 的参数：155,150,105,45

细扫帧数和ROI区域个数不对应啊。

今日任务：
1. realsense出来真实图像
2. 更改默认限位参数
3. 固化探测板运动参数
4. 展示ROI矩形区域
5. 限束器控制


细扫总是少两帧啊，不知道为什么。
别的都没问题，就是少帧。

# 2022.6.23

realsense出图

运动参数合集编写

高内聚低耦合

Nanocare DCT 拍摄界面实现，一键切换DCT/SDD

限束器控制以及校准实现。

寄存器沟通增加协议复位

DCT运动测试

# 2022.6.24
异步丢帧问题。

_rawImages是正面的缓冲数据

之后加了侧面的探测板也需要加侧面的缓冲数据。

1. 异步拍摄丢帧（减去丢三帧，时序是对了，但是细扫图不对啊。）
2. 寄存器增加复位协议（已实现。）
3. DCT、SDD快速切换（已实现。）
4. DCT运动，同步虚拟拍照测试
5. 添加限束器控制

用正面的ROI信息去校准侧面的ROI信息，因为正面和侧面的z位置是相同的。

# 2022.6.25
reset之后，初扫模组不移动了，估计是发的寄存器没有reset。
上位机只要寄存器有变化就会通知下位机，下位机来读取上位机寄存器的值。
如果读到的值是它想要的值，就会进行相应的操作。

树莓派开机自动启动rs_FDR

cd /home/realsense/rs_FDR
sudo ./run

1. 修改了树莓派开机启动realsense程序。

2. 改进了reset传给探测板参数的问题，reset恢复正常。

# 2022.6.28
1. realsense出正面和侧面图。
2. 跑一下全部数据的AI。

# 2022.6.29
1. 看EOS视频，了解整个操作的过程，对我们的软件有借鉴意义
2. realsense双线程（不是用双线程，多进程给我启发，让我用了单进程也弄好了。）
3. 线性模组速度优化控制噪声和总时长
4. C#实现云图测试拼接
5. C#实现3D展示和操作
6. 侧面模型图像处理＋算法优化（开发小程序去头尾）（图像灰度预处理，高清ROI与全图分开训练。）


重大发现：先启动serial1，再启动serial0，这个顺序是OK的。

如果失败，CTX或许要创建两个。
{"name":"polygon","all_points_x":[1552,1812,1782,1541],"all_points_y":[4378,4450,4629,4562]}

# 2022.6.30
1. 移动优化（第一步快），第二步（Vm降低，加速度。）（目前还不能移动，主要是modbus通信失败，推测是xbox主控板被拆的原因。）
2. 修改彩色图读取顺序（√）
3. 正侧面彩图优化，项目中实现正侧图像同时拼接（√）（有bug报错，第9张图报错）
4. 增加thumbnail功能。（√）
5. 增加AI标记区域图和预览图 （√）
6. 项目中导出云图（√）
7. 让AI算出z轴值匹配的正侧面ROI图。
8. 新增Meshlab弹出窗口

9. DCT断层多幅图像展示


# 2022.7.01

树莓派通信删除得在得到图片之后删除。在之前删除估计是有延时，把新生成的也删除了。‘
1. 拿到侧面的云图（√）
2. 融合代码（√）
3. 模组移动（没有XBOX控制板就是无法移动）（有了XBOX控制板就能移动了）
4. AI结果

# 2022.7.02
1. 探测板没有传数据是因为探测板与xbox主控板有根线未连接，因为数据先要传到XBOX主控板的。
2. 拿云图太阻塞，另开一个线程拿云图。
3. 打印下信息，看一下为什么拼接不成功。原来是Side检测到了就直接break了，没有等front也拼接。
4. 直接用thread.sleep会造成窗口假死
5. 把自动增强加上，看下效果。
6. 保存细扫的半身图，用于后续AI处理。
7. 目前来看他彩色拍摄间隔是同样的啊，好像不会因为我拍摄的少了，间隔就增大。
8. 第二个界面的AI不会处理，没有将图片resize成1024就直接运行AI了，少了这个步骤，后面出来的图标签就对用不上。不对，还是resize了的。
9. AI 长宽不对应，导致lable不对应，已修复。
10. DCT虚拟拍摄实现
11. 优化软件布局

# 2022.7.04
1. AI辅助诊断，为什么上移那个字母会重复啊？
2. 彩图拼接


体验云图对齐。ply好像就是一些点坐标，而stl文件是一些三角形？！！

目前的上移下移功能：（前提是初始AI一定是从T1开始顺序生成直到L5。）
上移：当不足17根时，全部骨头标记不动，同时向上生成新骨头，并将新骨头命名为T1，依次往下命名；
      当有17根时，最下面骨头标记消失，其余骨头标记不动，同时向上生成新骨头标记，并将新骨头命名为T1，依次往下命名。
	  
下移：第一根骨头标记消失，同时最下面生成新的骨头，并将新的第一根骨头命名为T1，依次往下命名。

调试键隐藏，AI窗口等待时间减少，下一步直接载入全图，隐藏载图按钮，AI计算时直接切换，ROI文件和视图文件。cobb角标记变红色。

导出csv文件

# 2022.7.05
1. 外推优化
2. 丰富报告内容
3. 优化云图融合以及转换效果，可以尝试使用registration
4. 侧面AI跑通

又是label的方向和导出需要的方向是有一点点反的。
自动内推越多，误差越大。


# 2022.7.06
1. 手工创建一个脊椎模型，与视觉图贴合(√)
2. 优化云图融合以及转换视觉效果（√）
3. GUI整理 增加剂量计算(√)，增加总时长统计（√），调整字体一致（√）
4. 丰富报告内容：增加病人信息（√）
5. 外推优化，和样本数据比对，找到建模失败原因，matlab的sample启动。

优化DCTProcessing页面。

初扫最后一图拼接。

demo0的数据要改一下。

# 2022.7.07

程序控制串口。

调试软件。


# 2022.7.08
1. 截取颈部和腰部图，AI重新训练。肩部这里必须要多一点剂量啊。
2. 开机报错bug，观察树莓派。
3. 外推优化，只外推中心点。（用脊椎骨之间的统计规律来外推。）
4. 找到脊椎骨3D建模失败原因。
5. 用C#处理ply云图。
6. ply云图自动配准。
7. 图像识别头部。
8. 图像自动识别头部、肩部、并画出上限位置和肩限位。

AI模型代码细节还不太懂啊。

docker技术封装虚拟环境。

# 2022.7.09
1. 侧面AI优化，test可视化结果
2. 外推优化
3. C#处理云图
4. 图像识别上限位和肩限位
5. 骨盆，腿骨建模，旋转孔。

AI训练图片要resize成1024×1024吗？不是，我倒是觉得上位机给AI预测的图片应该首先resize成1024×512。

# 2022.7.11
1. 侧面AI，模型大小分析，数据分析。
原论文使用580张图片作为数据集，60%训练（348张），20%验证（116张），20%测试（116张）。

虽然现在过拟合了，但是试过了将res34减到res18，效果变差了，果然减小模型复杂度不是个好办法
现在试一下加入正则化系数，然后加入了正则化系数之后果然可以，基本都能正确识别。

接下来学习边缘检测的方法。

灰度值就是亮度值，灰度图像就是它只有亮度值（相当于RGB分量都相等，这时候三通道也就只有一个通道了）。
彩色图有RGB三个分量，每个分量的亮度不相同，这样就形成了彩色的视觉。
彩色图转灰度图其实就是把彩色图每个分量的亮度进行一个取样，RGB三个分量的亮度依据某种规则形成一个亮度，这样就形成了灰度图
每个像素有不同的位，位数越多，能够表示的亮度值就越多，单个像素占用内存空间就越大。
而分辨率是一张图片长宽有多少个像素点（像素数量，而不表示位数），但是不同图片每个像素点不一定相同（位数，长宽都不一定相同）。

### 边缘检测算法：
边缘检测算法是指利用灰度值的不连续性质，以灰度突变为基础分割出目标区域。
在灰度产生突变的区域，可以用边缘检测算子来对特征进行提取。

算子就是如何描述这些突变的算法。

算子的种类
1. 一阶算子：
    基于一阶微分的算子，也称基于搜索的算子，首先通过一阶导数计算边缘强度，
	（这里应该是涉及取样了吧，因为图像像素是离散的，而在数学分析中只有某点连续才有导数）
	然后采用梯度的方向来对边缘的局部方向进行寻找，同时根据该方向来寻找出局部梯度值模的最大值，
	由此定位边缘。
	图像的边缘区域，像素值会发生“跳跃”，对这些像素求导，在其一阶导数在边缘位置求极值，这就是Sobel算子使用的原理：极值处就是边缘。
	
2. 二阶算子
   就是一阶导数极值点的地方，二阶导数为0。

### 一阶算子分析
一阶微分算子进行边缘检测的思路大致就是通过制定大小的核（kernal）与图像进行卷积，将得到的梯度进行平方和
或者最大值作为新的梯度赋值给对应的像素点，不同的一阶微分算子主要的不同在于其算子即核的元素不同以及核的大小不一样

因为图像是一个面，就相当于是灰度值关于x，y两个方向的函数，要求某一点的导数，则是各个方向的偏导数的平方和再进行开方计算。

关注的是某个点，但是其余点要为了算梯度也要包括进来的，所有就有了卷积核。

差分是变化量（离散叫差分，连续叫微分？！！）啊。

Roberts是以对角线作为差分的方向。但其实在我的这个项目中以水平或者垂直方向为差分方向就可以了。

Canny 边缘检测算法是被业界公认的性能最为优良的边缘检测算法之一。
Canny算法不是像Roberts、Prewitt、Sobel等这样简单梯度算子或锐化模板，它是在梯度算子基础上，引入了一种能获得抗噪性能好、定位精度高的单像素边缘的计算策略。
Canny 算子，在一阶微分的基础上，增加了非最大值抑制和双阈值检测，是边缘检测算子中最常用的一种，常被其它算子作为标准算子来进行优劣比较。


就是我感觉工业界目前有点和学术界脱节（当然大厂不一定，但是很多小厂应该是有点脱节的。）
那么是不是说学术界本身也存在脱节现象呢，当然这个现象主要是学科之间的脱节，某些学科的难点，
可能借助别的学科的知识，就能轻易解决，但是由于交叉学科的发展不足，很多问题没有得以解决。

# 2022.7.12
1. 边缘检测(√)
2. 跑通matlab建模
3. 三维重建了解一下

网格（Mesh），是计算机图形学中用来表示模型的定点与多边形集合，其中多边形通常为三角形。
在现实世界中，物体的表面都是光滑的曲面，但在3D渲染过程中，物体表面由大量细小的多边形组成，
当使用的多边形数量足够多，最终可以渲染出非常光滑的曲面。
比如我们可以使用大量三角形网格来组成一个球体模型，模型的表面包含了三角形顶点，法线，切线等信息。

# 2022.7.13
1. 三维建模
2. 彩图实时拼接（先给下位机一个pulse，再给树莓派一个START1，得到图之后先拼接显示，再判断有无头部，如果有头部，则停止，反之继续。）
   这个得图该怎么？上位机要，还是树莓派主动传？！！按理说应该是树莓派主动传。
   
   树莓派等pulse增加超时自动退出功能。
   差不多已经实现（除了人脸识别，然后停止没有实现之外。如果中途停止，需要改变最大帧数。）
   
3D重建算法原理：
基于深度摄像机的三维重建技术，
##### 双目、多目视觉
双目视觉主要利用左右相机得到的两幅校正图像找到左右图片的匹配点，然后根据集合原理恢复出环境的三维信息。
但该方法难点在于左右相机图片的匹配，匹配地不精确都会影响最后算法成像的效果。
多目视觉采用三个或三个以上摄像机来提高匹配的精度，缺点也很明显，需要消耗更多的时间，实时性也更差。

sfm和多视图几何等经典算法作为入门三维重建领域的基础永远都不会过时。

三维重建领域主要的数据格式有四种：
1. 深度图：2D图片，每个像素记录从视点到物体的距离，以灰度图表示，越近越黑。
2. 体素（voxel）
3. 点云：每个点有三维坐标，也可包括色彩，反射强度信息。
4. 网格（mesh）多边形网格


# 2022.7.14
要它停止估计是停止不了了，那我就不让他停止，它继续拍，realsense也继续拍，但是我只要我要的。
1. 三维建模
2. C#调用python人脸识别（√），就是python中的路径得用绝对路径，因为调用python的路径还是在C#程序下面的。
3. 缩小成像脊椎图。（先不做，因为物理间距，不同体型人，不同脊柱弯曲程度，不同站位，都会改变，无法统一。）


继续三维建模，了解中心思想就行了，没必要事无巨细。

依据处理的数据形式不同将研究简要分为三部分：1.基于体素； 2.基于点云； 3.基于网格。
而基于深度图的三维重建算法暂时还没有，因为它更多的是用来在2D图像中可视化具体的三维信息而非处理数据。


# 2022.7.15
1. 三维重建三维重建三维建模三维建模

# 2022.7.18
1. C#GPU加速。
2. 自动画出上限位和肩限位。
3. AI识别，盆骨位腿骨线，旋转孔。
4. 3D建模：骨盆，大腿骨
5. 增加矫正界面
6. 解决开机报错bug（在TCP监听线程，有一个变量为null）
7. 使用C#处理对齐云图，人体和脊柱显示也不用meshlab，直接用C#显示3D图像。

# 2022.7.19

调试嵌入式程序

C#GPU加速

# 2022.7.20
1. 开发C#显示云图窗口。

步骤：
    在C++下成功实现点云图显示
	将C++导出为dll文件
	C#调用dll文件
	
	
# 2022.7.21
1. 确定彩图拼接时间
2. 尝试PCL展示云图

现在不能达到实时的目的，有几个解决办法：人脸检测另开一个线程，人脸检测和拼接显示同时进行，拼接用cuda（先测试不拼接的时长）。
有了一个稳定的错误之后就别再弄了。

# 2022.7.22
1. 实现彩图实时拼接
2. 实现头肩限位

修改realsense自动上传代码（√）
修改人脸识别程序，让其自动寻找本地图片，轮询，这本身应该自成一个线程。
修改上位机彩图实时拼接流程，彩图拼接显示为主线程，其余都为次要线程。
给人脸识别线程加上一个让线性模组停止运动的指令.

C#配置opencv

先实现一个小的demo,再去实现我自己的成功

opencv配置有问题Realease和debug不要同时配置，只配置一个，不然会串，导出为dll时会失败。

# 2022.7.25
调用ply云图。python的open3D


##### ply云图的粗配准，就是先下采样进行一波粗配准，但是这时候因为下采样了，所以配得不准。
      然后根据粗配准的结果，进行原图配准，这时候就能够配得很准了。
	  

重新训练正面图片AI